{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport time\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.metrics import accuracy_score, f1_score\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom transformers import BertTokenizer, BertModel\nfrom transformers import logging","metadata":{"execution":{"iopub.status.busy":"2023-01-15T00:47:44.623606Z","iopub.execute_input":"2023-01-15T00:47:44.623961Z","iopub.status.idle":"2023-01-15T00:47:44.631010Z","shell.execute_reply.started":"2023-01-15T00:47:44.623931Z","shell.execute_reply":"2023-01-15T00:47:44.629966Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/input/2022-inlp-final/train.json') as f:\n    train_data = json.load(f)\nwith open('/kaggle/input/2022-inlp-final/valid.json') as f:\n    valid_data = json.load(f)\nwith open('/kaggle/input/2022-inlp-final/test.json') as f:\n    test_data = json.load(f)\nwith open('/kaggle/input/2022-inlp-final/train_evidence_v3.json') as f:\n    train_data_evidence = json.load(f)\nwith open('/kaggle/input/2022-inlp-final/valid_evidence_v3.json') as f:\n    valid_data_evidence = json.load(f)\nwith open('/kaggle/input/2022-inlp-final/test_evidence_v3.json') as f:\n    test_data_evidence = json.load(f)\n\n    \nprint(f\"Training Dataset Size: {len(train_data)}\")\nprint(f\"Validation Dataset Size: {len(valid_data)}\")\nprint(f\"Testing Dataset Size: {len(test_data)}\")","metadata":{"execution":{"iopub.status.busy":"2023-01-15T00:47:44.633138Z","iopub.execute_input":"2023-01-15T00:47:44.634222Z","iopub.status.idle":"2023-01-15T00:47:44.983148Z","shell.execute_reply.started":"2023-01-15T00:47:44.634154Z","shell.execute_reply":"2023-01-15T00:47:44.982135Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Training Dataset Size: 16894\nValidation Dataset Size: 2360\nTesting Dataset Size: 2360\n","output_type":"stream"}]},{"cell_type":"code","source":"logging.set_verbosity_error()\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained('bert-base-uncased')\n\nclass_num = 3\narticle_num = 3\nbatch_size = 11\nsequence_length = 512\n\nnum_epoch = 3\nshow_freq = 20\nDEVICE = 'cuda'\n\ndef url_processing(url):\n    url = url.replace('/', ' ')\n    url = url.replace('.', ' ')\n    url = url.replace('-', ' ')\n    url = url.replace('?', ' ')\n    url = url.replace('&', ' ')\n    url = url.replace('=', ' ')\n    url = url.replace('_', ' ')\n    url = url.replace('%', ' ')\n    url = url.replace('+', ' ')\n    url = url.replace('#', ' ')\n    return url\n\ndef get_dataloader(all_data, all_evidence, shuffle=True, mode=\"train\"):\n    \n    all_input_ids = list()\n    all_input_mask = list()\n    all_segment_ids = list()\n    all_label_ids = list()\n    for data in tqdm(all_data):\n        claim = data['metadata']['claim']\n        file_id = str(data['metadata']['id'])\n\n        if mode != \"test\":\n            label = data['label']['rating']\n        \n        claimant = \"\" if data['metadata']['claimant'] is None else data['metadata']['claimant']\n        \n        premise_article_url = '/'.join(list(data['metadata']['premise_articles']))\n        premise_article_url = url_processing(premise_article_url)\n        \n        evidence = \"\" if len(all_evidence[file_id]) == 0 else \" . \".join(all_evidence[file_id])\n        \n        sentences = [claimant, premise_article_url, evidence]\n        \n        encoding = tokenizer(\n            text = [claim] * article_num, \n            text_pair = sentences + [\"\"] * (article_num - len(sentences)),\n            return_tensors = \"pt\",\n            padding = 'max_length',\n            truncation = 'only_second',\n            max_length = sequence_length\n        )\n\n        all_input_ids.append(encoding.input_ids.unsqueeze(0))\n        all_input_mask.append(encoding.attention_mask.unsqueeze(0))\n        all_segment_ids.append(encoding.token_type_ids.unsqueeze(0))\n        \n        if mode != \"test\":\n            all_label_ids.append(torch.tensor(label).unsqueeze(0))\n\n    all_input_ids = torch.cat(all_input_ids)\n    all_input_mask = torch.cat(all_input_mask)\n    all_segment_ids = torch.cat(all_segment_ids)\n    \n    if mode != \"test\":\n        all_label_ids = torch.cat(all_label_ids)\n    \n    if mode != \"test\":\n        dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n    else:\n        dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids)\n        \n    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n    \n    return loader","metadata":{"execution":{"iopub.status.busy":"2023-01-15T00:47:44.985437Z","iopub.execute_input":"2023-01-15T00:47:44.985854Z","iopub.status.idle":"2023-01-15T00:47:48.742639Z","shell.execute_reply.started":"2023-01-15T00:47:44.985814Z","shell.execute_reply":"2023-01-15T00:47:48.741623Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train_loader = get_dataloader(train_data, train_data_evidence, mode=\"train\")\nvalid_loader = get_dataloader(valid_data, valid_data_evidence, mode=\"valid\")","metadata":{"execution":{"iopub.status.busy":"2023-01-15T00:47:48.744049Z","iopub.execute_input":"2023-01-15T00:47:48.744409Z","iopub.status.idle":"2023-01-15T00:50:59.819956Z","shell.execute_reply.started":"2023-01-15T00:47:48.744371Z","shell.execute_reply":"2023-01-15T00:50:59.819034Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"100%|██████████| 16894/16894 [02:46<00:00, 101.65it/s]\n100%|██████████| 2360/2360 [00:24<00:00, 97.73it/s] \n","output_type":"stream"}]},{"cell_type":"code","source":"class BertForFactChecking(nn.Module):\n    def __init__(self):\n        super(BertForFactChecking, self).__init__()\n        \n        self.bert = bert_model\n        self.dropout = nn.Dropout()\n        self.dense1 = nn.Linear(768, 300)\n        self.dense2 = nn.Linear(300, 300)\n        self.classifier = nn.Linear(300, class_num)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        \n        input_ids = input_ids.view(-1, input_ids.size(-1))\n        attention_mask = attention_mask.view(-1, attention_mask.size(-1))\n        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1))\n        \n        outputs = self.bert(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n        )\n        \n        pooled_output = outputs[1]\n        pooled_output = self.dropout(pooled_output)\n        pooled_output = self.dense1(pooled_output)\n        pooled_output = self.dropout(pooled_output)\n        pooled_output = self.dense2(pooled_output)\n        logits = self.classifier(pooled_output)\n        reshaped_logits = logits.view(-1, article_num, class_num)\n        \n        return reshaped_logits.sum(1)","metadata":{"execution":{"iopub.status.busy":"2023-01-15T00:50:59.822587Z","iopub.execute_input":"2023-01-15T00:50:59.823036Z","iopub.status.idle":"2023-01-15T00:50:59.833864Z","shell.execute_reply.started":"2023-01-15T00:50:59.822999Z","shell.execute_reply":"2023-01-15T00:50:59.832877Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model = BertForFactChecking()\n\nfor name, param in model.named_parameters():\n    if name.startswith(\"bert.embeddings.\"):\n        param.requires_grad = False\n    if name.startswith(\"bert.encoder.layer.0.\"):\n        param.requires_grad = False\n    if name.startswith(\"bert.encoder.layer.1.\"):\n        param.requires_grad = False\n    if name.startswith(\"bert.encoder.layer.2.\"):\n        param.requires_grad = False\n    if name.startswith(\"bert.encoder.layer.3.\"):\n        param.requires_grad = False\n    if name.startswith(\"bert.encoder.layer.4.\"):\n        param.requires_grad = False\n    if name.startswith(\"bert.encoder.layer.5.\"):\n        param.requires_grad = False\n\nfor name, param in model.named_parameters():\n    print(name, param.requires_grad)\n    \nmodel = model.to(DEVICE)","metadata":{"execution":{"iopub.status.busy":"2023-01-15T01:04:06.285846Z","iopub.execute_input":"2023-01-15T01:04:06.286229Z","iopub.status.idle":"2023-01-15T01:04:06.307397Z","shell.execute_reply.started":"2023-01-15T01:04:06.286176Z","shell.execute_reply":"2023-01-15T01:04:06.306364Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"bert.embeddings.word_embeddings.weight False\nbert.embeddings.position_embeddings.weight False\nbert.embeddings.token_type_embeddings.weight False\nbert.embeddings.LayerNorm.weight False\nbert.embeddings.LayerNorm.bias False\nbert.encoder.layer.0.attention.self.query.weight False\nbert.encoder.layer.0.attention.self.query.bias False\nbert.encoder.layer.0.attention.self.key.weight False\nbert.encoder.layer.0.attention.self.key.bias False\nbert.encoder.layer.0.attention.self.value.weight False\nbert.encoder.layer.0.attention.self.value.bias False\nbert.encoder.layer.0.attention.output.dense.weight False\nbert.encoder.layer.0.attention.output.dense.bias False\nbert.encoder.layer.0.attention.output.LayerNorm.weight False\nbert.encoder.layer.0.attention.output.LayerNorm.bias False\nbert.encoder.layer.0.intermediate.dense.weight False\nbert.encoder.layer.0.intermediate.dense.bias False\nbert.encoder.layer.0.output.dense.weight False\nbert.encoder.layer.0.output.dense.bias False\nbert.encoder.layer.0.output.LayerNorm.weight False\nbert.encoder.layer.0.output.LayerNorm.bias False\nbert.encoder.layer.1.attention.self.query.weight False\nbert.encoder.layer.1.attention.self.query.bias False\nbert.encoder.layer.1.attention.self.key.weight False\nbert.encoder.layer.1.attention.self.key.bias False\nbert.encoder.layer.1.attention.self.value.weight False\nbert.encoder.layer.1.attention.self.value.bias False\nbert.encoder.layer.1.attention.output.dense.weight False\nbert.encoder.layer.1.attention.output.dense.bias False\nbert.encoder.layer.1.attention.output.LayerNorm.weight False\nbert.encoder.layer.1.attention.output.LayerNorm.bias False\nbert.encoder.layer.1.intermediate.dense.weight False\nbert.encoder.layer.1.intermediate.dense.bias False\nbert.encoder.layer.1.output.dense.weight False\nbert.encoder.layer.1.output.dense.bias False\nbert.encoder.layer.1.output.LayerNorm.weight False\nbert.encoder.layer.1.output.LayerNorm.bias False\nbert.encoder.layer.2.attention.self.query.weight False\nbert.encoder.layer.2.attention.self.query.bias False\nbert.encoder.layer.2.attention.self.key.weight False\nbert.encoder.layer.2.attention.self.key.bias False\nbert.encoder.layer.2.attention.self.value.weight False\nbert.encoder.layer.2.attention.self.value.bias False\nbert.encoder.layer.2.attention.output.dense.weight False\nbert.encoder.layer.2.attention.output.dense.bias False\nbert.encoder.layer.2.attention.output.LayerNorm.weight False\nbert.encoder.layer.2.attention.output.LayerNorm.bias False\nbert.encoder.layer.2.intermediate.dense.weight False\nbert.encoder.layer.2.intermediate.dense.bias False\nbert.encoder.layer.2.output.dense.weight False\nbert.encoder.layer.2.output.dense.bias False\nbert.encoder.layer.2.output.LayerNorm.weight False\nbert.encoder.layer.2.output.LayerNorm.bias False\nbert.encoder.layer.3.attention.self.query.weight False\nbert.encoder.layer.3.attention.self.query.bias False\nbert.encoder.layer.3.attention.self.key.weight False\nbert.encoder.layer.3.attention.self.key.bias False\nbert.encoder.layer.3.attention.self.value.weight False\nbert.encoder.layer.3.attention.self.value.bias False\nbert.encoder.layer.3.attention.output.dense.weight False\nbert.encoder.layer.3.attention.output.dense.bias False\nbert.encoder.layer.3.attention.output.LayerNorm.weight False\nbert.encoder.layer.3.attention.output.LayerNorm.bias False\nbert.encoder.layer.3.intermediate.dense.weight False\nbert.encoder.layer.3.intermediate.dense.bias False\nbert.encoder.layer.3.output.dense.weight False\nbert.encoder.layer.3.output.dense.bias False\nbert.encoder.layer.3.output.LayerNorm.weight False\nbert.encoder.layer.3.output.LayerNorm.bias False\nbert.encoder.layer.4.attention.self.query.weight False\nbert.encoder.layer.4.attention.self.query.bias False\nbert.encoder.layer.4.attention.self.key.weight False\nbert.encoder.layer.4.attention.self.key.bias False\nbert.encoder.layer.4.attention.self.value.weight False\nbert.encoder.layer.4.attention.self.value.bias False\nbert.encoder.layer.4.attention.output.dense.weight False\nbert.encoder.layer.4.attention.output.dense.bias False\nbert.encoder.layer.4.attention.output.LayerNorm.weight False\nbert.encoder.layer.4.attention.output.LayerNorm.bias False\nbert.encoder.layer.4.intermediate.dense.weight False\nbert.encoder.layer.4.intermediate.dense.bias False\nbert.encoder.layer.4.output.dense.weight False\nbert.encoder.layer.4.output.dense.bias False\nbert.encoder.layer.4.output.LayerNorm.weight False\nbert.encoder.layer.4.output.LayerNorm.bias False\nbert.encoder.layer.5.attention.self.query.weight False\nbert.encoder.layer.5.attention.self.query.bias False\nbert.encoder.layer.5.attention.self.key.weight False\nbert.encoder.layer.5.attention.self.key.bias False\nbert.encoder.layer.5.attention.self.value.weight False\nbert.encoder.layer.5.attention.self.value.bias False\nbert.encoder.layer.5.attention.output.dense.weight False\nbert.encoder.layer.5.attention.output.dense.bias False\nbert.encoder.layer.5.attention.output.LayerNorm.weight False\nbert.encoder.layer.5.attention.output.LayerNorm.bias False\nbert.encoder.layer.5.intermediate.dense.weight False\nbert.encoder.layer.5.intermediate.dense.bias False\nbert.encoder.layer.5.output.dense.weight False\nbert.encoder.layer.5.output.dense.bias False\nbert.encoder.layer.5.output.LayerNorm.weight False\nbert.encoder.layer.5.output.LayerNorm.bias False\nbert.encoder.layer.6.attention.self.query.weight True\nbert.encoder.layer.6.attention.self.query.bias True\nbert.encoder.layer.6.attention.self.key.weight True\nbert.encoder.layer.6.attention.self.key.bias True\nbert.encoder.layer.6.attention.self.value.weight True\nbert.encoder.layer.6.attention.self.value.bias True\nbert.encoder.layer.6.attention.output.dense.weight True\nbert.encoder.layer.6.attention.output.dense.bias True\nbert.encoder.layer.6.attention.output.LayerNorm.weight True\nbert.encoder.layer.6.attention.output.LayerNorm.bias True\nbert.encoder.layer.6.intermediate.dense.weight True\nbert.encoder.layer.6.intermediate.dense.bias True\nbert.encoder.layer.6.output.dense.weight True\nbert.encoder.layer.6.output.dense.bias True\nbert.encoder.layer.6.output.LayerNorm.weight True\nbert.encoder.layer.6.output.LayerNorm.bias True\nbert.encoder.layer.7.attention.self.query.weight True\nbert.encoder.layer.7.attention.self.query.bias True\nbert.encoder.layer.7.attention.self.key.weight True\nbert.encoder.layer.7.attention.self.key.bias True\nbert.encoder.layer.7.attention.self.value.weight True\nbert.encoder.layer.7.attention.self.value.bias True\nbert.encoder.layer.7.attention.output.dense.weight True\nbert.encoder.layer.7.attention.output.dense.bias True\nbert.encoder.layer.7.attention.output.LayerNorm.weight True\nbert.encoder.layer.7.attention.output.LayerNorm.bias True\nbert.encoder.layer.7.intermediate.dense.weight True\nbert.encoder.layer.7.intermediate.dense.bias True\nbert.encoder.layer.7.output.dense.weight True\nbert.encoder.layer.7.output.dense.bias True\nbert.encoder.layer.7.output.LayerNorm.weight True\nbert.encoder.layer.7.output.LayerNorm.bias True\nbert.encoder.layer.8.attention.self.query.weight True\nbert.encoder.layer.8.attention.self.query.bias True\nbert.encoder.layer.8.attention.self.key.weight True\nbert.encoder.layer.8.attention.self.key.bias True\nbert.encoder.layer.8.attention.self.value.weight True\nbert.encoder.layer.8.attention.self.value.bias True\nbert.encoder.layer.8.attention.output.dense.weight True\nbert.encoder.layer.8.attention.output.dense.bias True\nbert.encoder.layer.8.attention.output.LayerNorm.weight True\nbert.encoder.layer.8.attention.output.LayerNorm.bias True\nbert.encoder.layer.8.intermediate.dense.weight True\nbert.encoder.layer.8.intermediate.dense.bias True\nbert.encoder.layer.8.output.dense.weight True\nbert.encoder.layer.8.output.dense.bias True\nbert.encoder.layer.8.output.LayerNorm.weight True\nbert.encoder.layer.8.output.LayerNorm.bias True\nbert.encoder.layer.9.attention.self.query.weight True\nbert.encoder.layer.9.attention.self.query.bias True\nbert.encoder.layer.9.attention.self.key.weight True\nbert.encoder.layer.9.attention.self.key.bias True\nbert.encoder.layer.9.attention.self.value.weight True\nbert.encoder.layer.9.attention.self.value.bias True\nbert.encoder.layer.9.attention.output.dense.weight True\nbert.encoder.layer.9.attention.output.dense.bias True\nbert.encoder.layer.9.attention.output.LayerNorm.weight True\nbert.encoder.layer.9.attention.output.LayerNorm.bias True\nbert.encoder.layer.9.intermediate.dense.weight True\nbert.encoder.layer.9.intermediate.dense.bias True\nbert.encoder.layer.9.output.dense.weight True\nbert.encoder.layer.9.output.dense.bias True\nbert.encoder.layer.9.output.LayerNorm.weight True\nbert.encoder.layer.9.output.LayerNorm.bias True\nbert.encoder.layer.10.attention.self.query.weight True\nbert.encoder.layer.10.attention.self.query.bias True\nbert.encoder.layer.10.attention.self.key.weight True\nbert.encoder.layer.10.attention.self.key.bias True\nbert.encoder.layer.10.attention.self.value.weight True\nbert.encoder.layer.10.attention.self.value.bias True\nbert.encoder.layer.10.attention.output.dense.weight True\nbert.encoder.layer.10.attention.output.dense.bias True\nbert.encoder.layer.10.attention.output.LayerNorm.weight True\nbert.encoder.layer.10.attention.output.LayerNorm.bias True\nbert.encoder.layer.10.intermediate.dense.weight True\nbert.encoder.layer.10.intermediate.dense.bias True\nbert.encoder.layer.10.output.dense.weight True\nbert.encoder.layer.10.output.dense.bias True\nbert.encoder.layer.10.output.LayerNorm.weight True\nbert.encoder.layer.10.output.LayerNorm.bias True\nbert.encoder.layer.11.attention.self.query.weight True\nbert.encoder.layer.11.attention.self.query.bias True\nbert.encoder.layer.11.attention.self.key.weight True\nbert.encoder.layer.11.attention.self.key.bias True\nbert.encoder.layer.11.attention.self.value.weight True\nbert.encoder.layer.11.attention.self.value.bias True\nbert.encoder.layer.11.attention.output.dense.weight True\nbert.encoder.layer.11.attention.output.dense.bias True\nbert.encoder.layer.11.attention.output.LayerNorm.weight True\nbert.encoder.layer.11.attention.output.LayerNorm.bias True\nbert.encoder.layer.11.intermediate.dense.weight True\nbert.encoder.layer.11.intermediate.dense.bias True\nbert.encoder.layer.11.output.dense.weight True\nbert.encoder.layer.11.output.dense.bias True\nbert.encoder.layer.11.output.LayerNorm.weight True\nbert.encoder.layer.11.output.LayerNorm.bias True\nbert.pooler.dense.weight True\nbert.pooler.dense.bias True\ndense1.weight True\ndense1.bias True\ndense2.weight True\ndense2.bias True\nclassifier.weight True\nclassifier.bias True\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.utils.class_weight import compute_class_weight\n\nylabels = []\nfor task in train_data:\n    ylabels.append(task['label']['rating'])\nylabels = np.array(ylabels)\nprint(ylabels)\n\nclass_weights = compute_class_weight('balanced',classes=np.unique(ylabels),y=ylabels)\nclass_weights = torch.tensor(class_weights,dtype=torch.float)\nclass_weights = class_weights.to(DEVICE)\nprint(class_weights)","metadata":{"execution":{"iopub.status.busy":"2023-01-15T01:04:11.825150Z","iopub.execute_input":"2023-01-15T01:04:11.826474Z","iopub.status.idle":"2023-01-15T01:04:11.845722Z","shell.execute_reply.started":"2023-01-15T01:04:11.826428Z","shell.execute_reply":"2023-01-15T01:04:11.844616Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"[0 1 1 ... 2 2 2]\ntensor([0.8045, 0.8045, 1.9459], device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"loss = nn.CrossEntropyLoss(weight=class_weights)\noptimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n\nbest_valid_f1_score = 0.0\nfor epoch in range(num_epoch):\n    epoch_start_time = time.time()\n    train_loss, valid_loss = 0.0, 0.0\n    train_count, valid_count = 0.0, 0.0\n    train_true, valid_true = list(), list()\n    train_pred, valid_pred = list(), list()\n    \n    model.train()\n    for i, (input_ids, input_mask, segment_ids, label_ids) in enumerate(train_loader):\n        input_ids = input_ids.to(DEVICE)\n        input_mask = input_mask.to(DEVICE)\n        segment_ids = segment_ids.to(DEVICE)\n        label_ids = label_ids.to(DEVICE)\n        \n        outputs = model(\n            input_ids=input_ids, \n            token_type_ids=segment_ids, \n            attention_mask=input_mask,\n        )\n        batch_loss = loss(outputs, label_ids)\n        \n        batch_loss.backward()\n        optimizer.step()      \n        model.zero_grad()\n                \n        train_loss += batch_loss.item()\n        train_count += input_ids.shape[0]\n        train_true += label_ids.tolist()\n        train_pred += np.argmax(outputs.cpu().detach().numpy(), axis=1).tolist()\n        \n        if (i+1) % show_freq == 0 or (i+1) == len(train_loader):\n            train_acc = accuracy_score(train_true, train_pred)\n            train_f1_score = f1_score(train_true, train_pred, average='macro')\n            print('[{:02d}/{:02d} - {:04d}/{:04d}] '.format(epoch+1, num_epoch, i+1, len(train_loader))\n                + '{:2.2f} sec '.format(time.time() - epoch_start_time)\n                + 'Train Acc: {:3.2f}% Loss: {:3.4f} '.format(train_acc*100, train_loss/train_count)\n                + 'F1 Score: {:3.4f}'.format(train_f1_score)\n            )\n            \n    model.eval()    \n    for i, (input_ids, input_mask, segment_ids, label_ids) in enumerate(valid_loader):\n        input_ids = input_ids.to(DEVICE)\n        input_mask = input_mask.to(DEVICE)\n        segment_ids = segment_ids.to(DEVICE)\n        label_ids = label_ids.to(DEVICE)\n        \n        with torch.no_grad():\n            outputs = model(\n                input_ids=input_ids, \n                token_type_ids=segment_ids, \n                attention_mask=input_mask,\n            )\n            \n        batch_loss = loss(outputs, label_ids)\n        \n        valid_loss += batch_loss.item()\n        valid_count += input_ids.shape[0]\n        valid_true += label_ids.tolist()\n        valid_pred += np.argmax(outputs.cpu().detach().numpy(), axis=1).tolist()\n        \n        if (i+1) % show_freq == 0 or (i+1) == len(valid_loader):\n            valid_acc = accuracy_score(valid_true, valid_pred)\n            valid_f1_score = f1_score(valid_true, valid_pred, average='macro')\n            print('[{:02d}/{:02d} - {:04d}/{:04d}] '.format(epoch+1, num_epoch, i+1, len(valid_loader))\n                + '{:2.2f} sec '.format(time.time() - epoch_start_time)\n                + 'Valid Acc: {:3.2f}% Loss: {:3.4f} '.format(valid_acc*100, valid_loss/valid_count)\n                + 'F1 Score: {:3.4f}'.format(valid_f1_score)\n            )\n            \n    if best_valid_f1_score < valid_f1_score:\n        best_valid_f1_score = valid_f1_score\n        torch.save(model.state_dict(), 'bert_weight.pth')\n        \nprint(f\"Best Validation F1 Score: {best_valid_f1_score}\")\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-01-15T01:04:14.909019Z","iopub.execute_input":"2023-01-15T01:04:14.909396Z","iopub.status.idle":"2023-01-15T02:34:29.544132Z","shell.execute_reply.started":"2023-01-15T01:04:14.909364Z","shell.execute_reply":"2023-01-15T02:34:29.543091Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"[01/03 - 0020/1536] 22.01 sec Train Acc: 42.73% Loss: 0.0975 F1 Score: 0.3586\n[01/03 - 0040/1536] 43.98 sec Train Acc: 52.73% Loss: 0.0899 F1 Score: 0.4556\n[01/03 - 0060/1536] 66.00 sec Train Acc: 54.70% Loss: 0.0894 F1 Score: 0.4815\n[01/03 - 0080/1536] 87.98 sec Train Acc: 55.11% Loss: 0.0877 F1 Score: 0.5015\n[01/03 - 0100/1536] 109.98 sec Train Acc: 52.82% Loss: 0.0872 F1 Score: 0.5020\n[01/03 - 0120/1536] 131.96 sec Train Acc: 51.74% Loss: 0.0872 F1 Score: 0.4990\n[01/03 - 0140/1536] 153.93 sec Train Acc: 51.56% Loss: 0.0873 F1 Score: 0.4944\n[01/03 - 0160/1536] 175.93 sec Train Acc: 51.70% Loss: 0.0866 F1 Score: 0.4981\n[01/03 - 0180/1536] 197.93 sec Train Acc: 51.57% Loss: 0.0862 F1 Score: 0.4968\n[01/03 - 0200/1536] 219.91 sec Train Acc: 51.09% Loss: 0.0868 F1 Score: 0.4925\n[01/03 - 0220/1536] 241.91 sec Train Acc: 51.57% Loss: 0.0866 F1 Score: 0.4960\n[01/03 - 0240/1536] 263.90 sec Train Acc: 52.20% Loss: 0.0860 F1 Score: 0.5030\n[01/03 - 0260/1536] 285.88 sec Train Acc: 52.06% Loss: 0.0857 F1 Score: 0.5040\n[01/03 - 0280/1536] 307.91 sec Train Acc: 51.85% Loss: 0.0855 F1 Score: 0.5028\n[01/03 - 0300/1536] 329.91 sec Train Acc: 52.42% Loss: 0.0853 F1 Score: 0.5079\n[01/03 - 0320/1536] 351.90 sec Train Acc: 52.07% Loss: 0.0851 F1 Score: 0.5053\n[01/03 - 0340/1536] 373.93 sec Train Acc: 51.93% Loss: 0.0851 F1 Score: 0.5039\n[01/03 - 0360/1536] 395.92 sec Train Acc: 51.97% Loss: 0.0847 F1 Score: 0.5054\n[01/03 - 0380/1536] 417.94 sec Train Acc: 52.15% Loss: 0.0847 F1 Score: 0.5067\n[01/03 - 0400/1536] 439.94 sec Train Acc: 52.50% Loss: 0.0843 F1 Score: 0.5096\n[01/03 - 0420/1536] 461.94 sec Train Acc: 52.75% Loss: 0.0841 F1 Score: 0.5115\n[01/03 - 0440/1536] 483.93 sec Train Acc: 53.06% Loss: 0.0837 F1 Score: 0.5149\n[01/03 - 0460/1536] 505.92 sec Train Acc: 53.04% Loss: 0.0834 F1 Score: 0.5158\n[01/03 - 0480/1536] 527.91 sec Train Acc: 53.30% Loss: 0.0833 F1 Score: 0.5180\n[01/03 - 0500/1536] 549.88 sec Train Acc: 53.07% Loss: 0.0833 F1 Score: 0.5166\n[01/03 - 0520/1536] 571.85 sec Train Acc: 53.22% Loss: 0.0833 F1 Score: 0.5184\n[01/03 - 0540/1536] 593.84 sec Train Acc: 53.45% Loss: 0.0832 F1 Score: 0.5203\n[01/03 - 0560/1536] 615.85 sec Train Acc: 53.56% Loss: 0.0831 F1 Score: 0.5215\n[01/03 - 0580/1536] 637.86 sec Train Acc: 53.51% Loss: 0.0832 F1 Score: 0.5209\n[01/03 - 0600/1536] 659.83 sec Train Acc: 53.50% Loss: 0.0831 F1 Score: 0.5213\n[01/03 - 0620/1536] 681.83 sec Train Acc: 53.58% Loss: 0.0831 F1 Score: 0.5222\n[01/03 - 0640/1536] 703.82 sec Train Acc: 53.66% Loss: 0.0830 F1 Score: 0.5232\n[01/03 - 0660/1536] 725.81 sec Train Acc: 53.72% Loss: 0.0830 F1 Score: 0.5234\n[01/03 - 0680/1536] 747.81 sec Train Acc: 53.84% Loss: 0.0827 F1 Score: 0.5254\n[01/03 - 0700/1536] 769.80 sec Train Acc: 54.08% Loss: 0.0825 F1 Score: 0.5277\n[01/03 - 0720/1536] 791.80 sec Train Acc: 54.09% Loss: 0.0824 F1 Score: 0.5274\n[01/03 - 0740/1536] 813.79 sec Train Acc: 54.25% Loss: 0.0824 F1 Score: 0.5286\n[01/03 - 0760/1536] 835.77 sec Train Acc: 54.15% Loss: 0.0823 F1 Score: 0.5282\n[01/03 - 0780/1536] 857.74 sec Train Acc: 54.43% Loss: 0.0823 F1 Score: 0.5303\n[01/03 - 0800/1536] 879.73 sec Train Acc: 54.51% Loss: 0.0824 F1 Score: 0.5311\n[01/03 - 0820/1536] 901.73 sec Train Acc: 54.58% Loss: 0.0823 F1 Score: 0.5318\n[01/03 - 0840/1536] 923.74 sec Train Acc: 54.59% Loss: 0.0823 F1 Score: 0.5317\n[01/03 - 0860/1536] 945.74 sec Train Acc: 54.82% Loss: 0.0823 F1 Score: 0.5337\n[01/03 - 0880/1536] 967.73 sec Train Acc: 54.89% Loss: 0.0821 F1 Score: 0.5343\n[01/03 - 0900/1536] 989.73 sec Train Acc: 54.86% Loss: 0.0821 F1 Score: 0.5344\n[01/03 - 0920/1536] 1011.72 sec Train Acc: 55.02% Loss: 0.0820 F1 Score: 0.5359\n[01/03 - 0940/1536] 1033.71 sec Train Acc: 55.12% Loss: 0.0819 F1 Score: 0.5366\n[01/03 - 0960/1536] 1055.71 sec Train Acc: 55.36% Loss: 0.0817 F1 Score: 0.5387\n[01/03 - 0980/1536] 1077.69 sec Train Acc: 55.55% Loss: 0.0816 F1 Score: 0.5405\n[01/03 - 1000/1536] 1099.71 sec Train Acc: 55.64% Loss: 0.0815 F1 Score: 0.5420\n[01/03 - 1020/1536] 1121.71 sec Train Acc: 55.84% Loss: 0.0814 F1 Score: 0.5432\n[01/03 - 1040/1536] 1143.70 sec Train Acc: 55.84% Loss: 0.0814 F1 Score: 0.5432\n[01/03 - 1060/1536] 1165.69 sec Train Acc: 55.83% Loss: 0.0815 F1 Score: 0.5430\n[01/03 - 1080/1536] 1187.68 sec Train Acc: 55.93% Loss: 0.0815 F1 Score: 0.5437\n[01/03 - 1100/1536] 1209.68 sec Train Acc: 55.88% Loss: 0.0815 F1 Score: 0.5430\n[01/03 - 1120/1536] 1231.64 sec Train Acc: 55.92% Loss: 0.0815 F1 Score: 0.5437\n[01/03 - 1140/1536] 1253.64 sec Train Acc: 55.93% Loss: 0.0815 F1 Score: 0.5438\n[01/03 - 1160/1536] 1275.64 sec Train Acc: 55.89% Loss: 0.0814 F1 Score: 0.5438\n[01/03 - 1180/1536] 1297.65 sec Train Acc: 55.92% Loss: 0.0814 F1 Score: 0.5442\n[01/03 - 1200/1536] 1319.65 sec Train Acc: 55.91% Loss: 0.0814 F1 Score: 0.5441\n[01/03 - 1220/1536] 1341.65 sec Train Acc: 55.86% Loss: 0.0813 F1 Score: 0.5435\n[01/03 - 1240/1536] 1363.74 sec Train Acc: 55.92% Loss: 0.0812 F1 Score: 0.5444\n[01/03 - 1260/1536] 1385.74 sec Train Acc: 56.08% Loss: 0.0811 F1 Score: 0.5454\n[01/03 - 1280/1536] 1407.75 sec Train Acc: 56.19% Loss: 0.0810 F1 Score: 0.5464\n[01/03 - 1300/1536] 1429.76 sec Train Acc: 56.24% Loss: 0.0809 F1 Score: 0.5470\n[01/03 - 1320/1536] 1451.78 sec Train Acc: 56.29% Loss: 0.0809 F1 Score: 0.5475\n[01/03 - 1340/1536] 1473.77 sec Train Acc: 56.23% Loss: 0.0810 F1 Score: 0.5469\n[01/03 - 1360/1536] 1495.78 sec Train Acc: 56.19% Loss: 0.0810 F1 Score: 0.5464\n[01/03 - 1380/1536] 1517.79 sec Train Acc: 56.25% Loss: 0.0809 F1 Score: 0.5471\n[01/03 - 1400/1536] 1539.81 sec Train Acc: 56.32% Loss: 0.0808 F1 Score: 0.5476\n[01/03 - 1420/1536] 1561.81 sec Train Acc: 56.39% Loss: 0.0808 F1 Score: 0.5483\n[01/03 - 1440/1536] 1583.83 sec Train Acc: 56.37% Loss: 0.0809 F1 Score: 0.5481\n[01/03 - 1460/1536] 1605.83 sec Train Acc: 56.46% Loss: 0.0808 F1 Score: 0.5491\n[01/03 - 1480/1536] 1627.86 sec Train Acc: 56.47% Loss: 0.0808 F1 Score: 0.5493\n[01/03 - 1500/1536] 1649.88 sec Train Acc: 56.46% Loss: 0.0808 F1 Score: 0.5493\n[01/03 - 1520/1536] 1671.91 sec Train Acc: 56.53% Loss: 0.0808 F1 Score: 0.5497\n[01/03 - 1536/1536] 1689.31 sec Train Acc: 56.54% Loss: 0.0809 F1 Score: 0.5495\n[01/03 - 0020/0215] 1700.01 sec Valid Acc: 63.18% Loss: 0.0759 F1 Score: 0.5851\n[01/03 - 0040/0215] 1710.70 sec Valid Acc: 60.23% Loss: 0.0803 F1 Score: 0.5581\n[01/03 - 0060/0215] 1721.39 sec Valid Acc: 60.76% Loss: 0.0816 F1 Score: 0.5624\n[01/03 - 0080/0215] 1732.08 sec Valid Acc: 60.80% Loss: 0.0808 F1 Score: 0.5579\n[01/03 - 0100/0215] 1742.77 sec Valid Acc: 60.18% Loss: 0.0813 F1 Score: 0.5521\n[01/03 - 0120/0215] 1753.46 sec Valid Acc: 60.61% Loss: 0.0806 F1 Score: 0.5590\n[01/03 - 0140/0215] 1764.15 sec Valid Acc: 61.49% Loss: 0.0793 F1 Score: 0.5666\n[01/03 - 0160/0215] 1774.84 sec Valid Acc: 61.14% Loss: 0.0797 F1 Score: 0.5641\n[01/03 - 0180/0215] 1785.53 sec Valid Acc: 60.61% Loss: 0.0800 F1 Score: 0.5594\n[01/03 - 0200/0215] 1796.22 sec Valid Acc: 60.91% Loss: 0.0798 F1 Score: 0.5592\n[01/03 - 0215/0215] 1804.00 sec Valid Acc: 60.97% Loss: 0.0801 F1 Score: 0.5596\n[02/03 - 0020/1536] 22.01 sec Train Acc: 70.00% Loss: 0.0734 F1 Score: 0.6377\n[02/03 - 0040/1536] 43.99 sec Train Acc: 66.59% Loss: 0.0723 F1 Score: 0.6142\n[02/03 - 0060/1536] 65.98 sec Train Acc: 67.73% Loss: 0.0712 F1 Score: 0.6431\n[02/03 - 0080/1536] 87.94 sec Train Acc: 66.25% Loss: 0.0708 F1 Score: 0.6362\n[02/03 - 0100/1536] 109.93 sec Train Acc: 64.91% Loss: 0.0714 F1 Score: 0.6256\n[02/03 - 0120/1536] 131.91 sec Train Acc: 64.62% Loss: 0.0720 F1 Score: 0.6282\n[02/03 - 0140/1536] 153.89 sec Train Acc: 64.61% Loss: 0.0720 F1 Score: 0.6290\n[02/03 - 0160/1536] 175.88 sec Train Acc: 63.64% Loss: 0.0723 F1 Score: 0.6179\n[02/03 - 0180/1536] 197.87 sec Train Acc: 62.88% Loss: 0.0730 F1 Score: 0.6084\n[02/03 - 0200/1536] 219.88 sec Train Acc: 62.41% Loss: 0.0733 F1 Score: 0.6040\n[02/03 - 0220/1536] 241.86 sec Train Acc: 62.56% Loss: 0.0735 F1 Score: 0.6052\n[02/03 - 0240/1536] 263.86 sec Train Acc: 62.61% Loss: 0.0730 F1 Score: 0.6050\n[02/03 - 0260/1536] 285.85 sec Train Acc: 62.90% Loss: 0.0727 F1 Score: 0.6066\n[02/03 - 0280/1536] 307.85 sec Train Acc: 62.82% Loss: 0.0731 F1 Score: 0.6072\n[02/03 - 0300/1536] 329.87 sec Train Acc: 62.33% Loss: 0.0733 F1 Score: 0.6037\n[02/03 - 0320/1536] 351.86 sec Train Acc: 62.59% Loss: 0.0731 F1 Score: 0.6057\n[02/03 - 0340/1536] 373.85 sec Train Acc: 62.25% Loss: 0.0732 F1 Score: 0.6023\n[02/03 - 0360/1536] 395.84 sec Train Acc: 62.10% Loss: 0.0730 F1 Score: 0.6014\n[02/03 - 0380/1536] 417.83 sec Train Acc: 62.01% Loss: 0.0730 F1 Score: 0.6014\n[02/03 - 0400/1536] 439.83 sec Train Acc: 61.95% Loss: 0.0731 F1 Score: 0.6016\n[02/03 - 0420/1536] 461.84 sec Train Acc: 61.56% Loss: 0.0734 F1 Score: 0.5990\n[02/03 - 0440/1536] 483.81 sec Train Acc: 61.38% Loss: 0.0736 F1 Score: 0.5969\n[02/03 - 0460/1536] 505.80 sec Train Acc: 61.44% Loss: 0.0733 F1 Score: 0.5979\n[02/03 - 0480/1536] 527.80 sec Train Acc: 61.25% Loss: 0.0735 F1 Score: 0.5960\n[02/03 - 0500/1536] 549.81 sec Train Acc: 61.20% Loss: 0.0737 F1 Score: 0.5961\n[02/03 - 0520/1536] 571.83 sec Train Acc: 61.05% Loss: 0.0738 F1 Score: 0.5944\n[02/03 - 0540/1536] 593.85 sec Train Acc: 61.03% Loss: 0.0738 F1 Score: 0.5946\n[02/03 - 0560/1536] 615.84 sec Train Acc: 61.10% Loss: 0.0738 F1 Score: 0.5956\n[02/03 - 0580/1536] 637.84 sec Train Acc: 61.32% Loss: 0.0736 F1 Score: 0.5981\n[02/03 - 0600/1536] 659.83 sec Train Acc: 61.12% Loss: 0.0737 F1 Score: 0.5965\n[02/03 - 0620/1536] 681.83 sec Train Acc: 61.10% Loss: 0.0737 F1 Score: 0.5965\n[02/03 - 0640/1536] 703.82 sec Train Acc: 61.09% Loss: 0.0738 F1 Score: 0.5966\n[02/03 - 0660/1536] 725.81 sec Train Acc: 61.07% Loss: 0.0737 F1 Score: 0.5961\n[02/03 - 0680/1536] 747.82 sec Train Acc: 61.00% Loss: 0.0738 F1 Score: 0.5958\n[02/03 - 0700/1536] 769.82 sec Train Acc: 60.82% Loss: 0.0740 F1 Score: 0.5944\n[02/03 - 0720/1536] 791.79 sec Train Acc: 60.80% Loss: 0.0738 F1 Score: 0.5951\n[02/03 - 0740/1536] 813.78 sec Train Acc: 60.93% Loss: 0.0737 F1 Score: 0.5965\n[02/03 - 0760/1536] 835.76 sec Train Acc: 60.98% Loss: 0.0736 F1 Score: 0.5972\n[02/03 - 0780/1536] 857.77 sec Train Acc: 61.04% Loss: 0.0735 F1 Score: 0.5977\n[02/03 - 0800/1536] 879.76 sec Train Acc: 61.05% Loss: 0.0738 F1 Score: 0.5975\n[02/03 - 0820/1536] 901.75 sec Train Acc: 60.90% Loss: 0.0738 F1 Score: 0.5963\n[02/03 - 0840/1536] 923.74 sec Train Acc: 60.92% Loss: 0.0738 F1 Score: 0.5968\n[02/03 - 0860/1536] 945.74 sec Train Acc: 60.91% Loss: 0.0738 F1 Score: 0.5966\n[02/03 - 0880/1536] 967.71 sec Train Acc: 60.87% Loss: 0.0739 F1 Score: 0.5962\n[02/03 - 0900/1536] 989.70 sec Train Acc: 61.01% Loss: 0.0737 F1 Score: 0.5974\n[02/03 - 0920/1536] 1011.70 sec Train Acc: 61.04% Loss: 0.0737 F1 Score: 0.5974\n[02/03 - 0940/1536] 1033.70 sec Train Acc: 61.03% Loss: 0.0737 F1 Score: 0.5979\n[02/03 - 0960/1536] 1055.71 sec Train Acc: 61.06% Loss: 0.0738 F1 Score: 0.5979\n[02/03 - 0980/1536] 1077.74 sec Train Acc: 61.04% Loss: 0.0738 F1 Score: 0.5974\n[02/03 - 1000/1536] 1099.72 sec Train Acc: 61.07% Loss: 0.0737 F1 Score: 0.5978\n[02/03 - 1020/1536] 1121.73 sec Train Acc: 61.11% Loss: 0.0737 F1 Score: 0.5984\n[02/03 - 1040/1536] 1143.73 sec Train Acc: 60.93% Loss: 0.0738 F1 Score: 0.5964\n[02/03 - 1060/1536] 1165.77 sec Train Acc: 61.03% Loss: 0.0736 F1 Score: 0.5976\n[02/03 - 1080/1536] 1187.80 sec Train Acc: 60.98% Loss: 0.0737 F1 Score: 0.5971\n[02/03 - 1100/1536] 1209.81 sec Train Acc: 61.12% Loss: 0.0736 F1 Score: 0.5985\n[02/03 - 1120/1536] 1231.83 sec Train Acc: 61.11% Loss: 0.0736 F1 Score: 0.5983\n[02/03 - 1140/1536] 1253.81 sec Train Acc: 61.14% Loss: 0.0736 F1 Score: 0.5986\n[02/03 - 1160/1536] 1275.81 sec Train Acc: 61.21% Loss: 0.0735 F1 Score: 0.5990\n[02/03 - 1180/1536] 1297.83 sec Train Acc: 61.30% Loss: 0.0734 F1 Score: 0.6000\n[02/03 - 1200/1536] 1319.83 sec Train Acc: 61.28% Loss: 0.0735 F1 Score: 0.5998\n[02/03 - 1220/1536] 1341.84 sec Train Acc: 61.25% Loss: 0.0735 F1 Score: 0.5994\n[02/03 - 1240/1536] 1363.82 sec Train Acc: 61.28% Loss: 0.0736 F1 Score: 0.5995\n[02/03 - 1260/1536] 1385.82 sec Train Acc: 61.39% Loss: 0.0734 F1 Score: 0.6007\n[02/03 - 1280/1536] 1407.83 sec Train Acc: 61.41% Loss: 0.0733 F1 Score: 0.6010\n[02/03 - 1300/1536] 1429.84 sec Train Acc: 61.41% Loss: 0.0733 F1 Score: 0.6009\n[02/03 - 1320/1536] 1451.84 sec Train Acc: 61.45% Loss: 0.0734 F1 Score: 0.6009\n[02/03 - 1340/1536] 1473.86 sec Train Acc: 61.55% Loss: 0.0732 F1 Score: 0.6017\n[02/03 - 1360/1536] 1495.84 sec Train Acc: 61.66% Loss: 0.0732 F1 Score: 0.6025\n[02/03 - 1380/1536] 1517.87 sec Train Acc: 61.63% Loss: 0.0732 F1 Score: 0.6022\n[02/03 - 1400/1536] 1539.90 sec Train Acc: 61.62% Loss: 0.0732 F1 Score: 0.6025\n[02/03 - 1420/1536] 1561.90 sec Train Acc: 61.61% Loss: 0.0731 F1 Score: 0.6026\n[02/03 - 1440/1536] 1583.92 sec Train Acc: 61.64% Loss: 0.0731 F1 Score: 0.6029\n[02/03 - 1460/1536] 1605.91 sec Train Acc: 61.59% Loss: 0.0732 F1 Score: 0.6025\n[02/03 - 1480/1536] 1627.94 sec Train Acc: 61.62% Loss: 0.0731 F1 Score: 0.6024\n[02/03 - 1500/1536] 1649.94 sec Train Acc: 61.66% Loss: 0.0731 F1 Score: 0.6026\n[02/03 - 1520/1536] 1671.93 sec Train Acc: 61.64% Loss: 0.0731 F1 Score: 0.6026\n[02/03 - 1536/1536] 1689.31 sec Train Acc: 61.61% Loss: 0.0732 F1 Score: 0.6022\n[02/03 - 0020/0215] 1700.00 sec Valid Acc: 62.73% Loss: 0.0748 F1 Score: 0.5555\n[02/03 - 0040/0215] 1710.70 sec Valid Acc: 63.41% Loss: 0.0787 F1 Score: 0.5753\n[02/03 - 0060/0215] 1721.39 sec Valid Acc: 62.42% Loss: 0.0800 F1 Score: 0.5665\n[02/03 - 0080/0215] 1732.08 sec Valid Acc: 61.36% Loss: 0.0809 F1 Score: 0.5593\n[02/03 - 0100/0215] 1742.77 sec Valid Acc: 62.55% Loss: 0.0793 F1 Score: 0.5720\n[02/03 - 0120/0215] 1753.46 sec Valid Acc: 63.03% Loss: 0.0785 F1 Score: 0.5816\n[02/03 - 0140/0215] 1764.15 sec Valid Acc: 62.99% Loss: 0.0791 F1 Score: 0.5850\n[02/03 - 0160/0215] 1774.84 sec Valid Acc: 63.01% Loss: 0.0792 F1 Score: 0.5881\n[02/03 - 0180/0215] 1785.53 sec Valid Acc: 62.02% Loss: 0.0792 F1 Score: 0.5766\n[02/03 - 0200/0215] 1796.23 sec Valid Acc: 61.41% Loss: 0.0796 F1 Score: 0.5720\n[02/03 - 0215/0215] 1804.00 sec Valid Acc: 61.36% Loss: 0.0796 F1 Score: 0.5724\n[03/03 - 0020/1536] 22.00 sec Train Acc: 65.91% Loss: 0.0702 F1 Score: 0.6276\n[03/03 - 0040/1536] 43.99 sec Train Acc: 67.95% Loss: 0.0653 F1 Score: 0.6624\n[03/03 - 0060/1536] 66.00 sec Train Acc: 69.39% Loss: 0.0640 F1 Score: 0.6748\n[03/03 - 0080/1536] 88.01 sec Train Acc: 70.11% Loss: 0.0609 F1 Score: 0.6851\n[03/03 - 0100/1536] 109.99 sec Train Acc: 69.55% Loss: 0.0615 F1 Score: 0.6822\n[03/03 - 0120/1536] 131.99 sec Train Acc: 69.47% Loss: 0.0607 F1 Score: 0.6826\n[03/03 - 0140/1536] 153.98 sec Train Acc: 69.87% Loss: 0.0604 F1 Score: 0.6868\n[03/03 - 0160/1536] 175.97 sec Train Acc: 69.55% Loss: 0.0607 F1 Score: 0.6837\n[03/03 - 0180/1536] 197.97 sec Train Acc: 69.39% Loss: 0.0606 F1 Score: 0.6839\n[03/03 - 0200/1536] 219.95 sec Train Acc: 69.09% Loss: 0.0609 F1 Score: 0.6817\n[03/03 - 0220/1536] 241.94 sec Train Acc: 69.34% Loss: 0.0607 F1 Score: 0.6838\n[03/03 - 0240/1536] 263.91 sec Train Acc: 69.09% Loss: 0.0613 F1 Score: 0.6829\n[03/03 - 0260/1536] 285.89 sec Train Acc: 68.92% Loss: 0.0612 F1 Score: 0.6816\n[03/03 - 0280/1536] 307.89 sec Train Acc: 69.19% Loss: 0.0611 F1 Score: 0.6834\n[03/03 - 0300/1536] 329.89 sec Train Acc: 69.48% Loss: 0.0604 F1 Score: 0.6864\n[03/03 - 0320/1536] 351.87 sec Train Acc: 69.29% Loss: 0.0606 F1 Score: 0.6847\n[03/03 - 0340/1536] 373.89 sec Train Acc: 69.39% Loss: 0.0603 F1 Score: 0.6861\n[03/03 - 0360/1536] 395.87 sec Train Acc: 69.34% Loss: 0.0601 F1 Score: 0.6852\n[03/03 - 0380/1536] 417.88 sec Train Acc: 69.64% Loss: 0.0599 F1 Score: 0.6883\n[03/03 - 0400/1536] 439.89 sec Train Acc: 69.80% Loss: 0.0599 F1 Score: 0.6896\n[03/03 - 0420/1536] 461.89 sec Train Acc: 69.76% Loss: 0.0600 F1 Score: 0.6887\n[03/03 - 0440/1536] 483.88 sec Train Acc: 69.71% Loss: 0.0601 F1 Score: 0.6877\n[03/03 - 0460/1536] 505.88 sec Train Acc: 69.58% Loss: 0.0602 F1 Score: 0.6872\n[03/03 - 0480/1536] 527.87 sec Train Acc: 69.47% Loss: 0.0605 F1 Score: 0.6864\n[03/03 - 0500/1536] 549.85 sec Train Acc: 69.29% Loss: 0.0605 F1 Score: 0.6850\n[03/03 - 0520/1536] 571.86 sec Train Acc: 69.46% Loss: 0.0605 F1 Score: 0.6869\n[03/03 - 0540/1536] 593.86 sec Train Acc: 69.66% Loss: 0.0603 F1 Score: 0.6888\n[03/03 - 0560/1536] 615.89 sec Train Acc: 69.50% Loss: 0.0603 F1 Score: 0.6869\n[03/03 - 0580/1536] 637.89 sec Train Acc: 69.61% Loss: 0.0602 F1 Score: 0.6880\n[03/03 - 0600/1536] 659.88 sec Train Acc: 69.39% Loss: 0.0603 F1 Score: 0.6858\n[03/03 - 0620/1536] 681.90 sec Train Acc: 69.34% Loss: 0.0605 F1 Score: 0.6848\n[03/03 - 0640/1536] 703.87 sec Train Acc: 69.29% Loss: 0.0606 F1 Score: 0.6843\n[03/03 - 0660/1536] 725.86 sec Train Acc: 69.34% Loss: 0.0606 F1 Score: 0.6848\n[03/03 - 0680/1536] 747.89 sec Train Acc: 69.33% Loss: 0.0606 F1 Score: 0.6848\n[03/03 - 0700/1536] 769.90 sec Train Acc: 69.34% Loss: 0.0606 F1 Score: 0.6850\n[03/03 - 0720/1536] 791.91 sec Train Acc: 69.20% Loss: 0.0609 F1 Score: 0.6835\n[03/03 - 0740/1536] 813.92 sec Train Acc: 69.20% Loss: 0.0609 F1 Score: 0.6832\n[03/03 - 0760/1536] 835.91 sec Train Acc: 69.26% Loss: 0.0609 F1 Score: 0.6839\n[03/03 - 0780/1536] 857.94 sec Train Acc: 69.29% Loss: 0.0608 F1 Score: 0.6841\n[03/03 - 0800/1536] 879.93 sec Train Acc: 69.25% Loss: 0.0609 F1 Score: 0.6834\n[03/03 - 0820/1536] 901.91 sec Train Acc: 69.25% Loss: 0.0608 F1 Score: 0.6831\n[03/03 - 0840/1536] 923.90 sec Train Acc: 69.30% Loss: 0.0608 F1 Score: 0.6833\n[03/03 - 0860/1536] 945.89 sec Train Acc: 69.23% Loss: 0.0610 F1 Score: 0.6823\n[03/03 - 0880/1536] 967.91 sec Train Acc: 69.11% Loss: 0.0612 F1 Score: 0.6814\n[03/03 - 0900/1536] 989.92 sec Train Acc: 69.20% Loss: 0.0612 F1 Score: 0.6824\n[03/03 - 0920/1536] 1011.92 sec Train Acc: 69.14% Loss: 0.0612 F1 Score: 0.6822\n[03/03 - 0940/1536] 1033.93 sec Train Acc: 69.03% Loss: 0.0613 F1 Score: 0.6814\n[03/03 - 0960/1536] 1055.95 sec Train Acc: 68.94% Loss: 0.0612 F1 Score: 0.6810\n[03/03 - 0980/1536] 1077.95 sec Train Acc: 68.88% Loss: 0.0612 F1 Score: 0.6805\n[03/03 - 1000/1536] 1099.94 sec Train Acc: 68.82% Loss: 0.0611 F1 Score: 0.6799\n[03/03 - 1020/1536] 1121.93 sec Train Acc: 68.89% Loss: 0.0610 F1 Score: 0.6808\n[03/03 - 1040/1536] 1143.94 sec Train Acc: 68.90% Loss: 0.0611 F1 Score: 0.6808\n[03/03 - 1060/1536] 1165.96 sec Train Acc: 68.86% Loss: 0.0611 F1 Score: 0.6805\n[03/03 - 1080/1536] 1187.96 sec Train Acc: 68.83% Loss: 0.0610 F1 Score: 0.6806\n[03/03 - 1100/1536] 1209.98 sec Train Acc: 68.87% Loss: 0.0611 F1 Score: 0.6808\n[03/03 - 1120/1536] 1231.98 sec Train Acc: 68.90% Loss: 0.0611 F1 Score: 0.6810\n[03/03 - 1140/1536] 1254.00 sec Train Acc: 68.87% Loss: 0.0611 F1 Score: 0.6806\n[03/03 - 1160/1536] 1275.99 sec Train Acc: 68.79% Loss: 0.0612 F1 Score: 0.6802\n[03/03 - 1180/1536] 1298.00 sec Train Acc: 68.88% Loss: 0.0612 F1 Score: 0.6809\n[03/03 - 1200/1536] 1320.01 sec Train Acc: 68.88% Loss: 0.0612 F1 Score: 0.6808\n[03/03 - 1220/1536] 1341.99 sec Train Acc: 68.82% Loss: 0.0613 F1 Score: 0.6802\n[03/03 - 1240/1536] 1364.00 sec Train Acc: 68.83% Loss: 0.0612 F1 Score: 0.6801\n[03/03 - 1260/1536] 1386.00 sec Train Acc: 68.92% Loss: 0.0612 F1 Score: 0.6809\n[03/03 - 1280/1536] 1408.01 sec Train Acc: 68.85% Loss: 0.0613 F1 Score: 0.6802\n[03/03 - 1300/1536] 1430.00 sec Train Acc: 68.85% Loss: 0.0613 F1 Score: 0.6802\n[03/03 - 1320/1536] 1452.01 sec Train Acc: 68.86% Loss: 0.0612 F1 Score: 0.6805\n[03/03 - 1340/1536] 1474.00 sec Train Acc: 68.87% Loss: 0.0612 F1 Score: 0.6806\n[03/03 - 1360/1536] 1496.02 sec Train Acc: 68.86% Loss: 0.0612 F1 Score: 0.6804\n[03/03 - 1380/1536] 1518.01 sec Train Acc: 68.96% Loss: 0.0611 F1 Score: 0.6810\n[03/03 - 1400/1536] 1540.02 sec Train Acc: 69.03% Loss: 0.0611 F1 Score: 0.6816\n[03/03 - 1420/1536] 1562.04 sec Train Acc: 69.10% Loss: 0.0611 F1 Score: 0.6823\n[03/03 - 1440/1536] 1584.07 sec Train Acc: 69.11% Loss: 0.0611 F1 Score: 0.6825\n[03/03 - 1460/1536] 1606.08 sec Train Acc: 69.06% Loss: 0.0612 F1 Score: 0.6819\n[03/03 - 1480/1536] 1628.08 sec Train Acc: 69.04% Loss: 0.0612 F1 Score: 0.6816\n[03/03 - 1500/1536] 1650.09 sec Train Acc: 69.01% Loss: 0.0613 F1 Score: 0.6815\n[03/03 - 1520/1536] 1672.08 sec Train Acc: 69.00% Loss: 0.0613 F1 Score: 0.6813\n[03/03 - 1536/1536] 1689.46 sec Train Acc: 69.01% Loss: 0.0613 F1 Score: 0.6815\n[03/03 - 0020/0215] 1700.15 sec Valid Acc: 60.00% Loss: 0.0883 F1 Score: 0.5425\n[03/03 - 0040/0215] 1710.83 sec Valid Acc: 59.32% Loss: 0.0885 F1 Score: 0.5607\n[03/03 - 0060/0215] 1721.52 sec Valid Acc: 60.00% Loss: 0.0848 F1 Score: 0.5728\n[03/03 - 0080/0215] 1732.22 sec Valid Acc: 58.64% Loss: 0.0841 F1 Score: 0.5611\n[03/03 - 0100/0215] 1742.91 sec Valid Acc: 57.45% Loss: 0.0844 F1 Score: 0.5533\n[03/03 - 0120/0215] 1753.60 sec Valid Acc: 57.95% Loss: 0.0845 F1 Score: 0.5563\n[03/03 - 0140/0215] 1764.29 sec Valid Acc: 57.86% Loss: 0.0856 F1 Score: 0.5544\n[03/03 - 0160/0215] 1774.99 sec Valid Acc: 58.13% Loss: 0.0847 F1 Score: 0.5544\n[03/03 - 0180/0215] 1785.68 sec Valid Acc: 58.18% Loss: 0.0848 F1 Score: 0.5564\n[03/03 - 0200/0215] 1796.37 sec Valid Acc: 57.86% Loss: 0.0847 F1 Score: 0.5541\n[03/03 - 0215/0215] 1804.15 sec Valid Acc: 58.01% Loss: 0.0850 F1 Score: 0.5549\nBest Validation F1 Score: 0.5723510361261811\n","output_type":"stream"}]},{"cell_type":"code","source":"test_loader = get_dataloader(test_data, test_data_evidence, shuffle=False, mode=\"test\")","metadata":{"execution":{"iopub.status.busy":"2023-01-15T02:35:28.900928Z","iopub.execute_input":"2023-01-15T02:35:28.901322Z","iopub.status.idle":"2023-01-15T02:35:52.471128Z","shell.execute_reply.started":"2023-01-15T02:35:28.901286Z","shell.execute_reply":"2023-01-15T02:35:52.470046Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"100%|██████████| 2360/2360 [00:23<00:00, 100.31it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"model = BertForFactChecking()\nmodel = model.to(DEVICE)\nmodel.load_state_dict(torch.load('bert_weight.pth'))\n         \ntest_pred = list()\nfor (input_ids, input_mask, segment_ids) in tqdm(test_loader):\n    input_ids = input_ids.to(DEVICE)\n    input_mask = input_mask.to(DEVICE)\n    segment_ids = segment_ids.to(DEVICE)\n\n    with torch.no_grad():\n        outputs = model(\n            input_ids=input_ids, \n            token_type_ids=segment_ids, \n            attention_mask=input_mask,\n        )\n\n    test_pred += np.argmax(outputs.cpu().detach().numpy(), axis=1).tolist()","metadata":{"execution":{"iopub.status.busy":"2023-01-15T02:35:52.472840Z","iopub.execute_input":"2023-01-15T02:35:52.473224Z","iopub.status.idle":"2023-01-15T02:37:47.749314Z","shell.execute_reply.started":"2023-01-15T02:35:52.473173Z","shell.execute_reply":"2023-01-15T02:37:47.748154Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"100%|██████████| 215/215 [01:55<00:00,  1.87it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.DataFrame({\n    'id': [data['metadata']['id'] for data in test_data],\n    'rating': test_pred\n})\n\ndf.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-01-15T02:37:47.751460Z","iopub.execute_input":"2023-01-15T02:37:47.752328Z","iopub.status.idle":"2023-01-15T02:37:47.764592Z","shell.execute_reply.started":"2023-01-15T02:37:47.752290Z","shell.execute_reply":"2023-01-15T02:37:47.763466Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}